{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabd320c-6d39-4181-95c8-ef5e9f89908f",
   "metadata": {},
   "source": [
    "# Generate clinical plans from patient-physician audio interviews\n",
    "\n",
    "This notebook demonstrates how to generate clinical plans from patient-physician audio interviews using AWS Managed services and Claude 3 generalised large language model family.  \n",
    "\n",
    "## Prerequisites\n",
    "- Verify that model access to Anthropic's Claude 3 Sonnet and Haiku is granted to the account being used, see documentation here: [Amazon Bedrock Model Access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)\n",
    "\n",
    "## Instructions\n",
    "1. The notebook is designed to run with Amazon SageMaker Notebook Instance. For instructions on how to onboard to a Sagemaker Notebook Instances, refer to this [link](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "2. Update your SageMaker IAM role (created when you initially set up the Sagemaker Notebook Instance) to\n",
    " contain the following AWS managed policies:\n",
    "\n",
    "- [AmazonBedrockFullAccess](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonBedrockFullAccess.html)\n",
    "- [AmazonTranscribeFullAccess](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonTranscribeFullAccess.html) \n",
    "\n",
    "You can find the SageMaker IAM role attached to your Notebook Instance from the **Amazon SageMaker Console** -> **Notebook Instance** in the section **Permissions and encryption**, as shown below:\n",
    "\n",
    "![IAMROLE](assets/SageMaker-nbi-IAM-role.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589e87d-6ae2-483d-bace-726f9368020a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows how to use transcribe and diarize pre-recorded conversations between patients and physicians, and use Claude 3 model family to generate structured clinical notes. \n",
    "\n",
    "As shown in the architecture diagram below, this Jupyter Notebook orchestrates:\n",
    "\n",
    "1. The retrival of patient-physician medical interviews from a public location\n",
    "2. The upload to the default Sagemaker S3 bucket\n",
    "3. The execution of an **Amazon Transcribe** batch job to transcribe and diarize the recordings\n",
    "4. The preparation of the structured prompt to generate the clinical plan\n",
    "5. Generation of the clinical plan using the Claude 3 model family\n",
    "\n",
    "![Architecture](assets/clinicalplans_genai.001.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3ece0-0484-40d3-85c7-ee45087e872a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Update boto3 SDK to version **`1.33.0`** or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5b8f9-1e26-4be8-aa66-8d3a9e61e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install botocore boto3 awscli tscribe pandas ipython --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a17f4-2e5a-45af-b078-dcf25dd022f9",
   "metadata": {},
   "source": [
    "## 1. Batch Transcription Using Python SDK\n",
    "\n",
    "Setting up the environment with the AWS clients and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682cb45-ac03-4813-9a75-3a5b7bdab8d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:55:55.663104Z",
     "start_time": "2024-03-20T12:55:54.911842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import tscribe\n",
    "import pandas\n",
    "import datetime\n",
    "from IPython.display import display_markdown, Markdown, clear_output\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "s3 = boto3.client('s3', region)\n",
    "transcribe = boto3.client('transcribe', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471294-cbef-41a1-bcaa-16cd112be884",
   "metadata": {},
   "source": [
    "#### 1.1. Download the recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2b512-2c9d-4c10-8468-0be748883b98",
   "metadata": {},
   "source": [
    "We will use the sample recording published as part of the supplemental materials of the following paper \"Fareez, F., Parikh, T., Wavell, C. et al. A dataset of simulated patient-physician medical interviews with a focus on respiratory cases. Sci Data 9, 313 (2022). https://doi.org/10.1038/s41597-022-01423-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1b015-6256-4a94-b030-7429878cc380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:55:55.670916Z",
     "start_time": "2024-03-20T12:55:55.663319Z"
    }
   },
   "outputs": [],
   "source": [
    "!curl -L --output data.zip https://springernature.figshare.com/ndownloader/files/30598530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf055027-08be-4ad3-87b2-ef073258df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq -o data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d7c9c-ce26-44a3-8b06-5038ee68aaed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:55:55.670916Z",
     "start_time": "2024-03-20T12:55:55.663319Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"rawdata\"\n",
    "inputs = sagemaker_session.upload_data(path=\"Data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39162c19-eb04-4b4c-a158-3fa8655333c1",
   "metadata": {},
   "source": [
    "In the variable below, indicate the name of the recorded session you want to transcribe and summarise:  \n",
    "- **`[object_name]`**: file name including the extension (e.g. RES0037.mp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f8e56-845b-45eb-9e1c-f06a6efe0c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:57:10.008552Z",
     "start_time": "2024-03-20T12:57:09.999081Z"
    }
   },
   "outputs": [],
   "source": [
    "object_name = \"RES0038.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfa6ae-48de-43a9-bf41-1164cbf8303f",
   "metadata": {},
   "source": [
    "We will prefill the value of the `[job_name]` variable such to create unique Transcribe jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69da5a-c1a7-4dd2-9918-bf6ae86f3669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:57:12.949994Z",
     "start_time": "2024-03-20T12:57:12.941044Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "media_uri = \"s3://%s/%s/%s/%s\" % (bucket, prefix, \"Audio Recordings\", object_name)\n",
    "job_name = \"transcribe-%s-%s\" % (object_name.split(\".\")[0],timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf523a-dd5f-4173-8bd0-86dd9e971dca",
   "metadata": {},
   "source": [
    "#### 1.2. Starting an AWS Transcribe job\n",
    "Invoking **`start_transcription_job`** API to start a transcription job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d77a2-ac46-4e75-93fe-570dce45037e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T12:57:15.701613Z",
     "start_time": "2024-03-20T12:57:14.962568Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = transcribe.start_transcription_job(\n",
    "    TranscriptionJobName=job_name,\n",
    "    LanguageCode='en-US',\n",
    "    Media={\n",
    "        'MediaFileUri': str(media_uri)\n",
    "    },\n",
    "    OutputBucketName=bucket,\n",
    "    Settings={\n",
    "        'ShowSpeakerLabels': True,\n",
    "        'MaxSpeakerLabels': 2,\n",
    "        'ChannelIdentification': False\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c4f54-ceef-4e9f-904d-9affa2b8e59f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3. Checking job status\n",
    "\n",
    "The code below will invoke Transcribe **`get_transcription_job`** API to retrieve the status of the job we started in the previous step. If the status is not Completed or Failed, the code waits 5 seconds to retry until the job reaches a final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f35f2-0d64-44f9-b15b-a14fcac55406",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-20T12:57:17.510371Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    status = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"Job status: \" + status.get('TranscriptionJob').get('TranscriptionJobName'))\n",
    "\n",
    "start_time = status.get('TranscriptionJob').get('StartTime')\n",
    "completion_time = status.get('TranscriptionJob').get('CompletionTime')\n",
    "diff = completion_time - start_time\n",
    "\n",
    "print(\"Job duration: \" + str(diff))\n",
    "print(\"Transcription file: \" + status.get('TranscriptionJob').get('Transcript').get('TranscriptFileUri'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefabaca-dd93-444f-8316-1589eda7d61d",
   "metadata": {},
   "source": [
    "#### 1.4. Analysing the scribe results\n",
    "The code below will download the **`transcribe.json`** file generated by Transcribe, will parse the file and extract the diarised transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab07c9a-32f2-4479-874f-f6263267d788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:32:53.204194Z",
     "start_time": "2024-03-19T12:32:48.228374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcription_file = job_name + \".json\"\n",
    "\n",
    "transcription = s3.get_object(Bucket=bucket, Key=transcription_file)\n",
    "body = json.loads(transcription['Body'].read())\n",
    "\n",
    "s3.download_file(bucket, transcription_file, \"output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec58cb483baf100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:35:29.483130Z",
     "start_time": "2024-03-19T12:35:28.789875Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tscribe.write(\"output.json\", format=\"csv\", save_as=\"output.csv\")\n",
    "\n",
    "desired_width = 600\n",
    "pandas.set_option('display.width', desired_width)\n",
    "\n",
    "transcript = pandas.read_csv(\"output.csv\",  names=[\"line\", \"start_time\", \"end_time\", \"speaker\", \"comment\"], header=None, skiprows=1)\n",
    "interaction = [\"%s, %s: %s\" % (segment[0], segment[1],segment[2]) for segment in transcript[['line','speaker', 'comment']].values.tolist()]\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03593de0-fbbb-41fc-b096-944f47ca0d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a9371fcedbb55",
   "metadata": {},
   "source": [
    "## 2. Generate clinical notes using Claude model family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ded26-f3d7-422a-9f91-3128ef7a59ec",
   "metadata": {},
   "source": [
    "### 2.1. Prompt engineering\n",
    "Claude is trained to be a helpful, honest, and harmless assistant. It is used to speaking in dialogue, and you can instruct it in regular natural language requests as if you were making requests of a human.The quality of the instructions you give Claude can have a large effect on the quality of its outputs, especially for complex tasks. See https://docs.anthropic.com/claude/docs/intro-to-prompting to learn more about prompt engineering.\n",
    "\n",
    "Structured enterprise-grade prompts may contain the following sections: \n",
    "1. **Task context**\n",
    "1. Tone context\n",
    "1. Background data, documents, and images\n",
    "1. **Detailed task description & rules**\n",
    "1. Examples\n",
    "1. Conversation history\n",
    "1. Immediate task description or request\n",
    "1. Thinking step by step / take a deep breath\n",
    "\n",
    "In our scenario, we will use a simplified prompt (template) that will instruct the model to generate a structured summary of the transcribed conversation and indicate the lines in the transcript that support each claim. This summary is divided in the following sections: \n",
    "\n",
    "1. Chief complaint\n",
    "1. History of present illness\n",
    "1. Review of systems\n",
    "1. Past medical history\n",
    "1. Assessment\n",
    "1. Plan\n",
    "1. Physical examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb7fd52731c25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:33:08.990046Z",
     "start_time": "2024-03-19T12:33:08.986414Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = '''You will be reading a transcript of a recorded conversation between a physician and a patient. You will find the conversation within the transcript XML tags. Your goal is to summarise \n",
    "it, capture the most significative insights and propose the appropriate action plan under a section named ‘clinical plan’ that includes the following sections: Chief complaint; History of present \n",
    "illness; Review of systems; Past medical history; Assessment; Plan; Physical examination. Per each claim you make, you need to indicate which lines of the transcript supports it (please indicate \n",
    "only the line numbers within the tag <line></line>).\n",
    "<transcript>\n",
    "%s\n",
    "</transcript>\n",
    "''' % \"\\n\".join(interaction)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dadf56-58a2-41d0-8807-42a993fd1a39",
   "metadata": {},
   "source": [
    "### 2.2. Payload preparation and model invocation\n",
    "The new generation of Claude model only support the Messages API, hence we must format the body of our payload in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e3ac841095dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                }],\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this workshop during an AWS Instructor-Led lab, please uncomment the following line:\n",
    "#region = \"us-west-2\"\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9228475-c22b-45cb-9d7e-21ba990d0fa8",
   "metadata": {},
   "source": [
    "#### 2.2.1 Claude 3 Sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3e35a-68a4-404a-8b0a-a4cb738b0e04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-19T12:54:19.663698Z"
    }
   },
   "source": [
    "The Bedrock service generates the entire summary for the given prompt in a single output, this can be slow if the output contains large amount of tokens.\n",
    "\n",
    "Below we explore the option how we can use Bedrock to stream the output such that the user could start consuming it as it is being generated by the model. For this Bedrock supports invoke_model_with_response_stream API providing ResponseStream that streams the output in form of chunks.\n",
    "\n",
    "Instead of generating the entire output, Bedrock sends smaller chunks from the model. This can be displayed in a consumable manner as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a111e9d9f1217a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-19T12:54:19.663698Z"
    }
   },
   "outputs": [],
   "source": [
    "def teletype_model_response(stream):\n",
    "    output = []\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if chunk_obj['type'] == 'content_block_delta':\n",
    "                    text = chunk_obj['delta']['text']\n",
    "                    clear_output(wait=True)\n",
    "                    output.append(text)\n",
    "                    display_markdown(Markdown(''.join(output)))\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76374ea2-e418-4053-b138-727fa220b0ac",
   "metadata": {},
   "source": [
    "We will print the content of the response immediately as the first string is returned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37a6a3d7afdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "# response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "# response_body = json.loads(response[\"body\"].read())\n",
    "# completion = response_body[\"content\"][0][\"text\"]\n",
    "# print(completion)\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "teletype_model_response(response.get('body'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb0018-f7ec-4b67-a2f9-fc9ad1d48780",
   "metadata": {},
   "source": [
    "#### 2.2.2. Claude 3 Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251900b-f6a0-403b-bf22-b6d27acdc7aa",
   "metadata": {},
   "source": [
    "Let's print the response only when it is returned in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc53bcb812efaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T12:34:52.313385Z",
     "start_time": "2024-03-19T12:34:42.295628Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "completion = response_body[\"content\"][0][\"text\"]\n",
    "display_markdown(Markdown(''.join(completion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4c276def4d5fc",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
